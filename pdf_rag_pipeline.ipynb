{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "153d94c3",
      "metadata": {},
      "source": [
        "# PDF → Chunks → Embeddings → Vector DB → Retrieve + Generate\n",
        "\n",
        "This notebook builds a simple RAG pipeline over PDFs in `data non traite`:\n",
        "\n",
        "1. Load PDF text (page by page)\n",
        "2. Split into chunks\n",
        "3. Embed chunks with a sentence-transformers model\n",
        "4. Store embeddings in a FAISS index on disk\n",
        "5. Retrieve relevant chunks for a question and generate an answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "74b7a4ee",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18,\n",
              " [WindowsPath('c:/Users/m2l3k/Desktop/LLM/data non traite/50-Coding-Interview-Questions.pdf'),\n",
              "  WindowsPath('c:/Users/m2l3k/Desktop/LLM/data non traite/College_Success_-_WEB_zQGCJTr.pdf'),\n",
              "  WindowsPath('c:/Users/m2l3k/Desktop/LLM/data non traite/College_Success_Concise.pdf'),\n",
              "  WindowsPath('c:/Users/m2l3k/Desktop/LLM/data non traite/Competitive Programmer_s Handbook.pdf'),\n",
              "  WindowsPath('c:/Users/m2l3k/Desktop/LLM/data non traite/Cracking the coding interview 6th edition-1.pdf')])"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from pypdf import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "PROJECT_DIR = Path.cwd()\n",
        "DATA_DIR = PROJECT_DIR / \"data non traite\"\n",
        "STORE_DIR = PROJECT_DIR / \"vector_store\"\n",
        "\n",
        "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PDF_RECURSIVE = True\n",
        "MAX_PDFS: int | None = None\n",
        "\n",
        "if not DATA_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Missing folder: {DATA_DIR}\")\n",
        "\n",
        "candidates = DATA_DIR.rglob(\"*\") if PDF_RECURSIVE else DATA_DIR.glob(\"*\")\n",
        "pdf_paths = sorted([p for p in candidates if p.is_file() and p.suffix.lower() == \".pdf\"])\n",
        "if MAX_PDFS is not None:\n",
        "    pdf_paths = pdf_paths[:MAX_PDFS]\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "GEN_DEVICE = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "len(pdf_paths), pdf_paths[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b213e199",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b662647b7bbb4c5c87f72b3625b92ff8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Reading PDFs:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def read_pdf_pages(pdf_path: Path) -> list[dict[str, Any]]:\n",
        "    try:\n",
        "        reader = PdfReader(str(pdf_path))\n",
        "    except Exception:\n",
        "        return []\n",
        "    pages: list[dict[str, Any]] = []\n",
        "    for page_index, page in enumerate(reader.pages):\n",
        "        text = page.extract_text() or \"\"\n",
        "        text = text.replace(\"\\u00a0\", \" \").strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        pages.append(\n",
        "            {\n",
        "                \"source\": pdf_path.relative_to(DATA_DIR).as_posix(),\n",
        "                \"page\": page_index + 1,\n",
        "                \"text\": text,\n",
        "            }\n",
        "        )\n",
        "    return pages\n",
        "\n",
        "\n",
        "all_pages: list[dict[str, Any]] = []\n",
        "for p in tqdm(pdf_paths, desc=\"Reading PDFs\"):\n",
        "    all_pages.extend(read_pdf_pages(p))\n",
        "\n",
        "(\n",
        "    len(all_pages),\n",
        "    (all_pages[0][\"source\"] if all_pages else None),\n",
        "    (all_pages[0][\"page\"] if all_pages else None),\n",
        "    ((all_pages[0][\"text\"][:200]) if all_pages else None),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8f70e8b9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12603,\n",
              " 'College_Success_-_WEB_zQGCJTr.pdf',\n",
              " 3,\n",
              " 'College Success SENIOR CONTRIBUTING AUTHORS AMY BALDWIN, UNIVERSITY OF CENTRAL ARKANSAS')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CHUNK_SIZE = 900\n",
        "CHUNK_OVERLAP = 150\n",
        "\n",
        "def split_text(text: str, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP) -> list[str]:\n",
        "    if chunk_overlap >= chunk_size:\n",
        "        raise ValueError(\"chunk_overlap must be < chunk_size\")\n",
        "\n",
        "    normalized = \" \".join(text.split())\n",
        "    if len(normalized) <= chunk_size:\n",
        "        return [normalized]\n",
        "\n",
        "    chunks: list[str] = []\n",
        "    start = 0\n",
        "    while start < len(normalized):\n",
        "        end = min(start + chunk_size, len(normalized))\n",
        "\n",
        "        if end < len(normalized):\n",
        "            boundary = normalized.rfind(\" \", start, end)\n",
        "            if boundary != -1 and boundary > start + int(chunk_size * 0.6):\n",
        "                end = boundary\n",
        "\n",
        "        chunk = normalized[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        if end >= len(normalized):\n",
        "            break\n",
        "        start = max(0, end - chunk_overlap)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_pages(\n",
        "    pages: list[dict[str, Any]],\n",
        "    chunk_size: int = 900,\n",
        "    chunk_overlap: int = 150,\n",
        ") -> list[dict[str, Any]]:\n",
        "    out: list[dict[str, Any]] = []\n",
        "    for page in pages:\n",
        "        chunks = split_text(page[\"text\"], chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        for i, c in enumerate(chunks):\n",
        "            out.append(\n",
        "                {\n",
        "                    \"source\": page[\"source\"],\n",
        "                    \"page\": page[\"page\"],\n",
        "                    \"chunk\": c,\n",
        "                    \"chunk_index\": i,\n",
        "                }\n",
        "            )\n",
        "    return out\n",
        "\n",
        "\n",
        "chunks = chunk_pages(all_pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "(\n",
        "    len(chunks),\n",
        "    (chunks[0][\"source\"] if chunks else None),\n",
        "    (chunks[0][\"page\"] if chunks else None),\n",
        "    ((chunks[0][\"chunk\"][:200]) if chunks else None),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "46214e39",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 197/197 [00:58<00:00,  3.37it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((12603, 384), dtype('float32'))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "@dataclass\n",
        "class EmbeddingConfig:\n",
        "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    normalize: bool = True\n",
        "\n",
        "\n",
        "def embed_texts(texts: list[str], cfg: EmbeddingConfig) -> np.ndarray:\n",
        "    model = SentenceTransformer(cfg.model_name, device=DEVICE)\n",
        "    vectors = model.encode(texts, batch_size=64, show_progress_bar=True)\n",
        "    vectors = np.asarray(vectors, dtype=np.float32)\n",
        "    if cfg.normalize:\n",
        "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1.0\n",
        "        vectors = vectors / norms\n",
        "    return vectors\n",
        "\n",
        "\n",
        "emb_cfg = EmbeddingConfig()\n",
        "chunk_texts = [c[\"chunk\"] for c in chunks]\n",
        "embeddings = embed_texts(chunk_texts, emb_cfg)\n",
        "embeddings.shape, embeddings.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "50706537",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12603"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_faiss_index(vectors: np.ndarray) -> faiss.Index:\n",
        "    if vectors.ndim != 2:\n",
        "        raise ValueError(\"vectors must be 2D\")\n",
        "    dim = vectors.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(vectors)\n",
        "    return index\n",
        "\n",
        "\n",
        "index = build_faiss_index(embeddings)\n",
        "index.ntotal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "82a07557",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(WindowsPath('c:/Users/m2l3k/Desktop/LLM/vector_store/chunks.faiss'),\n",
              " WindowsPath('c:/Users/m2l3k/Desktop/LLM/vector_store/chunks.json'),\n",
              " WindowsPath('c:/Users/m2l3k/Desktop/LLM/vector_store/store_config.json'))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INDEX_PATH = STORE_DIR / \"chunks.faiss\"\n",
        "META_PATH = STORE_DIR / \"chunks.json\"\n",
        "CFG_PATH = STORE_DIR / \"store_config.json\"\n",
        "\n",
        "faiss.write_index(index, str(INDEX_PATH))\n",
        "\n",
        "with META_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(chunks, f, ensure_ascii=False)\n",
        "\n",
        "with CFG_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(\n",
        "        {\n",
        "            \"embedding_model\": emb_cfg.model_name,\n",
        "            \"normalize\": emb_cfg.normalize,\n",
        "            \"chunk_size\": CHUNK_SIZE,\n",
        "            \"chunk_overlap\": CHUNK_OVERLAP,\n",
        "        },\n",
        "        f,\n",
        "    )\n",
        "\n",
        "INDEX_PATH, META_PATH, CFG_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "58c753e6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Introduction_To_Computer_Science_-_WEB.pdf', 43, 0.756),\n",
              " ('Introduction_To_Computer_Science_-_WEB.pdf', 20, 0.723),\n",
              " ('Introduction_To_Computer_Science_-_WEB.pdf', 32, 0.709),\n",
              " ('Introduction_To_Computer_Science_-_WEB.pdf', 34, 0.707),\n",
              " ('Introduction_To_Computer_Science_-_WEB.pdf', 44, 0.703)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_store(store_dir: Path) -> tuple[faiss.Index, list[dict[str, Any]], dict[str, Any]]:\n",
        "    idx = faiss.read_index(str(store_dir / \"chunks.faiss\"))\n",
        "    meta = json.loads((store_dir / \"chunks.json\").read_text(encoding=\"utf-8\"))\n",
        "    cfg = json.loads((store_dir / \"store_config.json\").read_text(encoding=\"utf-8\"))\n",
        "    return idx, meta, cfg\n",
        "\n",
        "\n",
        "def retrieve(\n",
        "    question: str,\n",
        "    idx: faiss.Index,\n",
        "    meta: list[dict[str, Any]],\n",
        "    embed_model: SentenceTransformer,\n",
        "    top_k: int = 5,\n",
        ") -> list[dict[str, Any]]:\n",
        "    q_vec = embed_model.encode([question])\n",
        "    q_vec = np.asarray(q_vec, dtype=np.float32)\n",
        "    norms = np.linalg.norm(q_vec, axis=1, keepdims=True)\n",
        "    norms[norms == 0] = 1.0\n",
        "    q_vec = q_vec / norms\n",
        "    scores, ids = idx.search(q_vec, top_k)\n",
        "    out: list[dict[str, Any]] = []\n",
        "    for rank, (score, i) in enumerate(zip(scores[0].tolist(), ids[0].tolist())):\n",
        "        if i < 0:\n",
        "            continue\n",
        "        row = dict(meta[i])\n",
        "        row[\"score\"] = float(score)\n",
        "        row[\"rank\"] = rank\n",
        "        out.append(row)\n",
        "    return out\n",
        "\n",
        "\n",
        "idx, meta, cfg = load_store(STORE_DIR)\n",
        "embed_model = SentenceTransformer(cfg[\"embedding_model\"])\n",
        "\n",
        "hits = retrieve(\"What is computer science?\", idx, meta, embed_model, top_k=5)\n",
        "[(h[\"source\"], h[\"page\"], round(h[\"score\"], 3)) for h in hits]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0f650464",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('Based on the provided context, here\\'s the story of the computer:\\n\\nThe story of the computer begins with early personal computers like the Programma and the Alto. These early computers set the stage for the rapid expansion of computing in the workplace. By 1980, there were several microcomputers on the market that made computing more accessible to small businesses and even individuals.\\n\\nComputing capabilities had expanded to include color graphics, spreadsheets, and word processing programs. The market competition between Microsoft, HP, IBM, Apple, and others shaped the industry and our society.\\n\\nIn 1983, Time magazine recognized the computer as \"Machine of the Year,\" replacing its traditional \"Man of the Year.\" These early computers have evolved into today\\'s laptops, cell phones, tablets, and wearables.\\n\\nA computer is simply a programmable machine that can execute predefined lists of instructions and respond to new instructions. It has core features that remain the same despite changes in technology, including processing speed and data storage capabilities.\\n\\nThe technology behind computers has continued to evolve through innovations, offering users enhanced features, reduced costs, and increased operating speeds. Today\\'s computers are built around the same basic technology as early computers, but with many advancements and improvements.',\n",
              " [{'source': 'Foundations_of_Information_Systems_-_WEB_oNlbGYl.pdf',\n",
              "   'page': 23,\n",
              "   'score': 0.5308539867401123},\n",
              "  {'source': 'Workplace_Software_and_Skills_-_WEB_IlfJtcP.pdf',\n",
              "   'page': 21,\n",
              "   'score': 0.5257468223571777},\n",
              "  {'source': 'Workplace_Software_and_Skills_-_WEB_IlfJtcP.pdf',\n",
              "   'page': 38,\n",
              "   'score': 0.5121006965637207}])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def build_prompt(question: str, contexts: list[dict[str, Any]]) -> str:\n",
        "    max_context_chars = 3500\n",
        "    parts: list[str] = []\n",
        "    used = 0\n",
        "    for c in contexts:\n",
        "        part = f\"Source: {c['source']} (page {c['page']})\\n{c['chunk']}\"\n",
        "        if used + len(part) > max_context_chars:\n",
        "            break\n",
        "        parts.append(part)\n",
        "        used += len(part)\n",
        "    joined = \"\\n\\n\".join(parts)\n",
        "    return (\n",
        "        \"You are a helpful tutor. Use only the provided context. \"\n",
        "        \"If the context does not contain the answer, say you do not know.\\n\\n\"\n",
        "        f\"Context:\\n{joined}\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "\n",
        "USE_GITHUB_MODELS = True\n",
        "ANSWER_LANGUAGE = \"English\"\n",
        "\n",
        "GITHUB_ENDPOINT = \"https://models.github.ai/inference\"\n",
        "GITHUB_MODELS_URL = f\"{GITHUB_ENDPOINT}/chat/completions\"\n",
        "GITHUB_MODEL = \"meta/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "\n",
        "def build_messages(question: str, contexts: list[dict[str, Any]]) -> list[dict[str, str]]:\n",
        "    max_context_chars = 3500\n",
        "    parts: list[str] = []\n",
        "    used = 0\n",
        "    for c in contexts:\n",
        "        part = f\"Source: {c['source']} (page {c['page']})\\n{c['chunk']}\"\n",
        "        if used + len(part) > max_context_chars:\n",
        "            break\n",
        "        parts.append(part)\n",
        "        used += len(part)\n",
        "    joined = \"\\n\\n\".join(parts)\n",
        "    system = (\n",
        "        \"You are a helpful tutor. Use only the provided context. \"\n",
        "        \"If the context does not contain the answer, say you do not know. \"\n",
        "        f\"Answer in {ANSWER_LANGUAGE}.\"\n",
        "    )\n",
        "    user = f\"Context:\\n{joined}\\n\\nQuestion: {question}\"\n",
        "    return [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
        "\n",
        "\n",
        "def generate_with_github(question: str, contexts: list[dict[str, Any]]) -> str:\n",
        "    token = (os.environ.get(\"GITHUB_TOKEN\") or \"\").strip()\n",
        "    if not token:\n",
        "        raise RuntimeError(\"Missing GITHUB_TOKEN environment variable\")\n",
        "    messages = build_messages(question, contexts)\n",
        "    r = requests.post(\n",
        "        GITHUB_MODELS_URL,\n",
        "        headers={\n",
        "            \"Accept\": \"application/vnd.github+json\",\n",
        "            \"Authorization\": f\"Bearer {token}\",\n",
        "            \"X-GitHub-Api-Version\": \"2022-11-28\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        },\n",
        "        json={\n",
        "            \"model\": GITHUB_MODEL,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": 0.2,\n",
        "            \"max_tokens\": 512,\n",
        "        },\n",
        "        timeout=300,\n",
        "    )\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    return str(data[\"choices\"][0][\"message\"][\"content\"]).strip()\n",
        "\n",
        "\n",
        "def answer_question(question: str, top_k: int = 5) -> dict[str, Any]:\n",
        "    contexts = retrieve(question, idx, meta, embed_model, top_k=top_k)\n",
        "    out = generate_with_github(question, contexts)\n",
        "    sources = [\n",
        "        {\"source\": c[\"source\"], \"page\": c[\"page\"], \"score\": c[\"score\"]}\n",
        "        for c in contexts\n",
        "    ]\n",
        "    return {\"answer\": out, \"sources\": sources}\n",
        "\n",
        "\n",
        "RUN_DEMO = False\n",
        "if RUN_DEMO:\n",
        "    result = answer_question(\"give me the story of the computer ?\", top_k=6)\n",
        "    result[\"answer\"], result[\"sources\"][:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "live-qa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def live_qa(top_k: int = 6) -> None:\n",
        "    print(\"Type a question and press Enter. Type 'exit' to stop.\")\n",
        "    while True:\n",
        "        try:\n",
        "            q = input(\"Question: \").strip()\n",
        "        except EOFError:\n",
        "            return\n",
        "        except Exception as e:\n",
        "            if e.__class__.__name__ == \"StdinNotImplementedError\":\n",
        "                return\n",
        "            raise\n",
        "        if not q or q.lower() in {\"exit\", \"quit\"}:\n",
        "            return\n",
        "        r = answer_question(q, top_k=top_k)\n",
        "        print(\"\\nAnswer:\\n\" + r[\"answer\"])\n",
        "        print(\"\\nSources:\")\n",
        "        for s in r[\"sources\"]:\n",
        "            print(f\"- {s['source']} (page {s['page']}, score {s['score']:.3f})\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "RUN_LIVE = True\n",
        "if RUN_LIVE:\n",
        "    live_qa(top_k=6)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
